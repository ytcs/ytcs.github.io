---
published: true
layout: post
title: "Mechanistic Interpretability: Part 2 - The Superposition Hypothesis"
categories: machine-learning
date: 2025-05-25
---

In [Part 1]({% post_url 2025-05-25-mechanistic-interpretability-part-1 %}), we introduced mechanistic interpretability and the circuits paradigm, which posits that neural networks learn meaningful features and connect them into computational circuits. A key challenge to this vision arises when individual components of a network, like neurons, appear to respond to many unrelated concepts simultaneously. This phenomenon, known as **polysemanticity**, complicates the straightforward interpretation of individual neural units. The **superposition hypothesis** offers a compelling explanation for polysemanticity, suggesting that neural networks represent more features than their dimensionality would naively suggest by encoding them in a distributed, overlapping manner.

## Polysemanticity: The Challenge

Imagine a single neuron in a language model that activates strongly for inputs related to Shakespearean sonnets, but also for Python programming errors, and also for discussions about baking sourdough. Such a neuron is polysemantic. If this neuron is active, what specific concept is the model currently processing? Its activation alone is ambiguous.

If many neurons are polysemantic, reverse-engineering the specific algorithms the network uses becomes incredibly difficult. We need a way to understand how distinct concepts are represented, even if not by individual neurons.

## The Superposition Hypothesis Explained

The superposition hypothesis, prominently discussed in the context of Transformer models by researchers at Anthropic and elsewhere, proposes that neural networks learn to represent a large number of features ($$N$$) within a lower-dimensional activation space ($$\\mathbb{R}^{d_{\\text{model}}}$$, where $$d_{\\text{model}} < N$$). This is achieved by representing features not as individual neurons firing, but as **directions in activation space**.\n\nIf a model has $$d_{\\text{model}}$$ neurons in a layer, it has a $$d_{\\text{model}}$$-dimensional space of possible activation vectors. The hypothesis states:\n\n1.  **Features as Directions:** True, underlying conceptual features ($$\\mathbf{f}_1, \\mathbf{f}_2, \\dots, \\mathbf{f}_N$$) correspond to specific vector directions in this $$d_{\\text{model}}$$-dimensional space.\n2.  **Linear Encoding (Locally):** When a set of these features $$\\mathcal{S} = \\{\\mathbf{f}_{i_1}, \\mathbf{f}_{i_2}, \\dots, \\mathbf{f}_{i_k}\\}$$ are simultaneously present or active in the input, the model\'s activation vector $$\\mathbf{a} \\in \\mathbb{R}^{d_{\\text{model}}}$$ in that layer is approximately a linear combination of these feature vectors:\n    $$\\mathbf{a} \\approx c_1 \\mathbf{f}_{i_1} + c_2 \\mathbf{f}_{i_2} + \\dots + c_k \\mathbf{f}_{i_k}$$\n    where $$c_j$$ are scalar coefficients representing the intensity or presence of feature $$\\mathbf{f}_{i_j}$$.\n3.  **Non-Privileged Basis (Neuron Basis vs. Feature Basis):** The crucial insight is that these feature directions $$\\mathbf{f}_j$$ are generally *not* aligned with the standard basis vectors of the activation space (i.e., the directions corresponding to individual neurons firing). The neuron basis is an accident of architecture; the feature basis is what the network learns is useful.\n\n### Geometric Interpretation of Superposition\n\nImagine the activation space $$\mathbb{R}^{d_{\\text{model}}}$$: each neuron corresponds to one axis in this space. A feature $$\\mathbf{f}_j$$ is a vector in this space. If the number of true features $$N$$ is greater than the dimension of the space $$d_{\\text{model}}$$, these $$N$$ feature vectors cannot all be orthogonal. They must overlap.\n\nWhen an activation vector $$\\mathbf{a}$$ is formed by a sum of feature vectors, $$ \\mathbf{a} = \\sum c_j \\mathbf{f}_j $$, this vector $$\\mathbf{a}$$ will generally have non-zero projections onto many of the standard neuron axes, even if only a few features $$\\mathbf{f}_j$$ are active (i.e., have $$c_j \\neq 0$$).\n\nA single neuron\'s activation value is its component of the vector $$\\mathbf{a}$$ along its specific axis (e.g., for neuron $$k$$, its activation is $$a_k = \\mathbf{a} \\cdot \\mathbf{e}_k$$, where $$\\mathbf{e}_k$$ is the standard basis vector for that neuron). Because the feature vectors $$\\mathbf{f}_j$$ are not aligned with these axes $$\\mathbf{e}_k$$, a single feature $$\\mathbf{f}_j$$ can contribute to the activation of many neurons. Conversely, a single neuron $$k$$ can be active because its axis has a non-zero projection from multiple active feature vectors $$\\mathbf{f}_j$$.\n\nThis is polysemanticity from a geometric viewpoint: a neuron is active not because one specific feature it \"owns\" is active, but because the current linear combination of active *features* (which are directions) results in an overall activation vector that has a component along that neuron\'s axis.\n\n## Why Superposition?\n\n-   **Efficiency:** It allows the model to represent a vast number of potentially useful features (concepts, patterns, attributes) without requiring an equally vast number of neurons. This is a form of representational compression.\n-   **Flexibility:** New features can potentially be learned and added to the mix without drastically reorganizing existing representations, by finding new \"directions\" in the existing space.\n\n## Consequences of Superposition\n\n-   **Polysemantic Neurons:** As explained, if features are directions not aligned with neuron axes, then a single neuron can be activated by many different combinations of underlying features that happen to have a projection along its axis.\n-   **Difficulty of Direct Interpretation:** Looking at individual neuron activations becomes misleading. A highly active neuron doesn\'t necessarily mean one specific, interpretable concept is strongly present.\n-   **Need for Advanced Techniques:** To find the true, monosemantic features, we need techniques that can look beyond individual neuron activations and identify these underlying feature directions. This motivates methods like dictionary learning using sparse autoencoders (covered in [Part 4]({% post_url 2025-05-25-mechanistic-interpretability-part-4 %})).\n\n## Theoretical Support and Evidence\n\n-   **Toy Models:** Researchers have created small, controlled neural networks (toy models) where they can explicitly set the number of features to be learned and the dimensionality of the layers. These models demonstrate that when the number of features exceeds the layer dimension, the networks indeed learn to superpose them, and individual neurons become polysemantic. These models also show how factors like feature sparsity and geometry (e.g., how orthogonal features are) influence how superposition occurs.\n-   **Empirical Findings in Large Models:** While harder to prove definitively, evidence from probing and dictionary learning in large language models suggests that superposition is a widespread phenomenon.\n\n## Conclusion\n\nThe superposition hypothesis provides a powerful theoretical lens for understanding why individual neurons in complex models like Transformers are often polysemantic. By representing features as directions in activation space rather than tying them to individual neurons, models can efficiently encode a large number of concepts. This, however, necessitates moving beyond naive neuron-level interpretations and developing methods to uncover these underlying, superposed feature directions.\n\nUnderstanding superposition is crucial because it reshapes our approach to finding interpretable units within neural networks, guiding us towards techniques that can de-mix these overlapping signals and reveal the true semantic building blocks of the model\'s computations.\n\nNext, in [Part 3]({% post_url 2025-05-25-mechanistic-interpretability-part-3 %}), we will introduce the mathematical framework for analyzing Transformer circuits, which will be essential for understanding how these features and components interact.\n\n---\n\n## References and Further Reading\n\n-   **Elhage, N., et al.** (2022). [Toy Models of Superposition](https://transformer-circuits.pub/2022/toy_models/index.html). *Transformer Circuits Thread*. (This is a key paper for understanding the mechanics and theory of superposition).\n-   **Olah, C.** (2022). [Mechanistic Interpretability, Variables, and the Importance of Interpretable Bases](https://distill.pub/2022/mechanistic-interpretability-scope/). *Distill*. (Discusses the concept of features as directions).\n-   Original ideas about distributed representations also come from earlier work in connectionism, e.g., by Hinton, Rumelhart, McClelland.\n