---
published: true
layout: post
title: "Mechanistic Interpretability: Part 8 - Attention Head Circuits: Patterns and Functions"
categories: machine-learning
date: 2025-05-25
---

[Part 3]({% post_url 2025-05-25-mechanistic-interpretability-part-3 %}) of this series introduced the mathematical framework for deconstructing Transformer attention heads into Query-Key (QK) and Output-Value (OV) circuits. [Part 7]({% post_url 2025-05-25-mechanistic-interpretability-part-7 %}) discussed circuits more broadly, defining attention heads as crucial **meso-circuits**. Now, we delve deeper into the specific **attention patterns and information processing functions** implemented by individual attention heads within the larger Transformer architecture. Understanding these patterns is key to deciphering how Transformers process sequential information.

Recall that for an input token representation $$\mathbf{x} \in \mathbb{R}^{d_{\text{model}}}$$, an attention head, with weight matrices $$\mathbf{W}_Q, \mathbf{W}_K \in \mathbb{R}^{d_{\text{model}} \times d_{\text{head}}}$$ and $$\mathbf{W}_V \in \mathbb{R}^{d_{\text{model}} \times d_{\text{head}}}$$, $$\mathbf{W}_O \in \mathbb{R}^{d_{\text{head}} \times d_{\text{model}}}$$, computes:
- Queries: $$\mathbf{q} = \mathbf{x} \mathbf{W}_Q \in \mathbb{R}^{d_{\text{head}}}$$
- Keys: $$\mathbf{k} = \mathbf{x} \mathbf{W}_K \in \mathbb{R}^{d_{\text{head}}}$$
- Values: $$\mathbf{v} = \mathbf{x} \mathbf{W}_V \in \mathbb{R}^{d_{\text{head}}}$$

Attention scores $$\alpha_{ij} = \text{Softmax}_j(\frac{\mathbf{q}_i \mathbf{k}_j^T}{\sqrt{d_{\text{head}}}})$$ determine how much the query token $$i$$ attends to the key token $$j$$. The head's output contribution to the residual stream for token $$i$$ is $$\mathbf{o}_i = (\sum_j \alpha_{ij} \mathbf{v}_j) \mathbf{W}_O \in \mathbb{R}^{d_{\text{model}}}$$.

## The QK Circuit: Generating Diverse Attention Patterns

The QK circuit, primarily characterized by the effective weight matrix $$\mathbf{W}_{QK} = \mathbf{W}_Q \mathbf{W}_K^T \in \mathbb{R}^{d_{\text{model}} \times d_{\text{model}}}$$, is responsible for determining *where* a head directs its attention. The unnormalized attention score $$e_{ij} = \mathbf{x}_i \mathbf{W}_{QK} \mathbf{x}_j^T$$ between a query token $$i$$ (with representation $$\mathbf{x}_i$$) and a key token $$j$$ (with representation $$\mathbf{x}_j$$) dictates the strength of this connection. The diverse structures learned within different $$\mathbf{W}_{QK}$$ matrices give rise to a taxonomy of common attention patterns, each serving distinct computational roles:

**Previous Token Heads** are fundamental for capturing immediate local context. They specialize in attending strongly to the token immediately preceding the current token (e.g., token at position $$t$$ queries information from token at position $$t-1$$). Their mathematical signature often involves $$\mathbf{W}_{QK}$$ being structured to leverage differences in positional encodings. For instance, the query projection $$\mathbf{x}_t \mathbf{W}_Q$$ might become sensitive to aspects of the positional encoding unique to position $$t$$, while the key projection $$\mathbf{x}_{t-1} \mathbf{W}_K$$ similarly captures aspects of position $$t-1$$. The matrix $$\mathbf{W}_Q \mathbf{W}_K^T$$ would then be such that the dot product $(\mathbf{x}_t \mathbf{W}_Q) (\mathbf{x}_{t-1} \mathbf{W}_K)^T$$ is maximized. These heads are crucial for tasks like autoregressive language modeling, enabling predictions based on the most recent token and forming rudimentary n-gram-like statistics (e.g., bigrams).

**Positional or Fixed Offset Heads** generalize this concept by attending to tokens at various fixed relative offsets from the current token (e.g., $$t-k$$ for some small integer $$k$$), or to specific absolute positions within the sequence, most notably the beginning-of-sequence (BOS/CLS) token. The $$\mathbf{W}_{QK}$$ matrix in such heads interacts strongly with specific components of the positional encodings. For example, a BOS head might have its $$\mathbf{W}_Q$$ matrix tuned to respond to the unique positional encoding of the BOS token, or its $$\mathbf{W}_K$$ matrix might be structured to make the BOS token's key vector highly attractive to many query positions. BOS/CLS heads often serve to aggregate global information from across the sequence, while other fixed offset heads can capture skip-gram-like information or detect structural properties related to fixed distances in text (e.g., attending two positions back to check for a specific grammatical pattern).

**Same Token Heads**, also known as diagonal attention heads, primarily focus attention on the current token itself (i.e., position $$t$$ attends to position $$t$$). Mathematically, their $$\mathbf{W}_{QK}$$ matrix might be structured to produce high scores when the query and key positions are identical. This could occur if, for certain feature subspaces represented by a direction $$\mathbf{d}$$, $$\mathbf{d}^T \mathbf{W}_{QK} \mathbf{d}$$ is significantly larger than $$\mathbf{d}^T \mathbf{W}_{QK} \mathbf{d'}$$ where $$\mathbf{d'}$$ is orthogonal to $$\mathbf{d}$$ (and from a different position). Such heads can be instrumental in gathering and potentially amplifying or transforming features from the current token's own representation before further processing by the OV circuit or subsequent layers.

**Content-Based or Pattern-Matching Heads** offer more dynamic and flexible attention mechanisms. Instead of relying on fixed positions, they attend to tokens whose *content* (as encoded in their vector representations $$\mathbf{x}_j$$) matches a pattern or concept being sought by the query vector $$\mathbf{x}_i \mathbf{W}_Q$$. The matrix $$\mathbf{W}_{QK}$$ in these heads is structured to compute high similarity scores (e.g., via dot products) between query vectors that represent a certain type of semantic or syntactic information and key vectors from tokens that embody that information. Often, such $$\mathbf{W}_{QK}$$ matrices are found to be low-rank. If $$\mathbf{W}_{QK} = \sum_k \sigma_k \mathbf{u}_k \mathbf{v}_k^T$$ is its singular value decomposition, then the score contribution from the $$k$$-th component is $$\sigma_k (\mathbf{x}_i^T \mathbf{u}_k) (\mathbf{x}_j^T \mathbf{v}_k)$$. This formulation explicitly shows how the alignment of the query token's representation $$\mathbf{x}_i$$ with a left singular vector $$\mathbf{u}_k$$ (a "query pattern" direction) and the key token's representation $$\mathbf{x}_j$$ with a corresponding right singular vector $$\mathbf{v}_k$$ (a "key pattern" direction) determines the attention strength. These heads are functionally diverse, enabling the model to, for instance, locate all mentions of a previously introduced entity, find words semantically related to a query concept, or identify tokens matching a learned syntactic role, irrespective of their absolute or relative positions.

Finally, **Broadcast or Diffuse Heads** exhibit attention patterns that are spread very broadly, often with near-uniform attention weights distributed across many tokens in the context, or they may attend to no specific tokens (e.g., focusing only on padding tokens if present). Their mathematical signature might be a $$\mathbf{W}_{QK}$$ matrix that is close to a zero matrix, or one structured such that the pre-softmax scores $$e_{ij} / \sqrt{d_{\text{head}}}$$ are nearly constant for many pairs $$(i,j)$$, leading to $$alpha_{ij} \approx 1/N_{\text{context}}$$ where $$N_{\text{context}}$$ is the number of attended tokens. Alternatively, the query vectors $$\mathbf{x}_i \mathbf{W}_Q$$ produced by such heads might simply have very small magnitudes. These heads can effectively act as a passthrough for the residual stream if their OV circuit also performs minimal processing (i.e., is close to an identity transformation). They are sometimes interpreted as being effectively "turned off" or perhaps waiting for a very specific, rare trigger pattern. In other cases, they might contribute to a default aggregation or smoothing of information from the context.

## The OV Circuit: Processing and Routing Attended Information

Once the QK circuit has determined *where* the head should attend by producing attention weights $$\alpha_{ij}$$, the Output-Value (OV) circuit takes over. Defined by the effective matrix $$\mathbf{W}_{OV} = \mathbf{W}_V \mathbf{W}_O \in \mathbb{R}^{d_{\text{model}} \times d_{\text{model}}}$$, this circuit dictates *what information is moved* from the attended value vectors ($$\mathbf{v}_j = \mathbf{x}_j \mathbf{W}_V$$) and, crucially, *how this information is transformed* before being written back to the residual stream. The final output contribution is $$\mathbf{o}_i = (\sum_j \alpha_{ij} \mathbf{x}_j \mathbf{W}_V) \mathbf{W}_O$$.

Common functional roles implemented by the OV circuit include:

**Copying Information:** This is perhaps the most straightforward function. The OV circuit aims to transmit the attended information largely unchanged. Mathematically, this implies that for the subspace of features being attended to, the matrix $$\mathbf{W}_{OV}$$ acts approximately as a scaled identity matrix, i.e., $$\mathbf{W}_{OV} \approx c\mathbf{I}$$ for some scalar $$c$$. If a vector $$\mathbf{z}$$ lies in this feature subspace, then $$\mathbf{z} \mathbf{W}_{OV} \approx c \mathbf{z}$$. When combined with a specific attention pattern, this allows for targeted information routing. For instance, a previous token head (determined by the QK circuit) paired with a copying OV circuit will effectively copy the representation of token $$t-1$$ to the residual stream at position $$t$$.

**Feature Transformation or Extraction:** More sophisticated OV circuits perform specific linear transformations on the attended value vectors, rather than just copying them. In such cases, $$\mathbf{W}_{OV}$$ is not an identity matrix. It might project the aggregated, attended information $$\sum_j \alpha_{ij} \mathbf{v}_j$$ onto a specific subspace, effectively extracting or emphasizing certain features while diminishing others. If $$\mathbf{W}_{OV}$$ has a low effective rank, its operation can be seen as projecting the attended information onto a lower-dimensional subspace defined by its principal left singular vectors (if thinking of $$\mathbf{W}_{OV}$$ transforming column vectors) or right singular vectors (for row vectors as used here). For example, a content-based head might attend to all textual mentions of "dates," and its OV circuit could then transform these varied date representations into a standardized canonical format or extract a specific abstract feature like "is this date in the past?"

**Information Suppression or No-Op Output:** Some OV circuits may learn to effectively suppress the flow of information. This can occur if $$\mathbf{W}_{OV} \approx \mathbf{0}$$ (the zero matrix), or if the value vectors $$\mathbf{v}_j = \mathbf{x}_j \mathbf{W}_V$$ produced by the value projection matrix $$\mathbf{W}_V$$ are themselves consistently near zero for the types of inputs the head attends to. In such cases, even if the QK circuit generates a clear and specific attention pattern, the head writes little or nothing back to the residual stream, rendering it a near no-op in terms of its output contribution. However, the attention scores themselves could, in principle, still be read or utilized by other hypothetical components in more complex interpretability scenarios, though this is less common for standard model operations.

## Analyzing Head Behavior: Connecting Weights to Algorithmic Function

To gain a comprehensive understanding of an individual attention head's algorithmic role, it is essential to analyze both its QK and OV circuits in conjunction. Several analytical techniques aid this process:

-   **Singular Value Decomposition (SVD) of $$\mathbf{W}_{QK}$$ and $$\mathbf{W}_{OV}$$**: Performing SVD on these effective matrices can reveal their principal directions of transformation and their effective rank. The magnitudes of the singular values indicate the importance or strength of transformation along each corresponding singular vector direction. A rapid decay in singular values suggests that the matrix is low-rank and thus specializes in processing information within a smaller subspace. The singular vectors themselves identify these specific input and output feature directions that the matrix primarily operates on. For $$\mathbf{W}_{QK}$$, these directions in $$\mathbb{R}^{d_{\text{model}}}$$ show what types of query-information and key-information are being compared. For $$\mathbf{W}_{OV}$$, they show what types of information from attended tokens are read into the head (via $$\mathbf{W}_V$$) and what types of information are written out (via $$\mathbf{W}_O$$).

-   **Maximal Activating Examples for Query and Key Directions**: Once SVD (or other feature discovery methods) identifies important operational directions (e.g., singular vectors of $$\mathbf{W}_Q$$ or $$\mathbf{W}_K$$), examining dataset examples that maximally activate these directions helps to assign semantic labels to them. This provides insight into *what kinds* of input features or concepts cause high activations in the query and key vectors along these critical processing pathways.

-   **Probing the $$\mathbf{W}_{OV}$$ Transformation**: To understand how $$\mathbf{W}_{OV}$$ transforms information, one can test its effect on known or hypothesized input feature directions. If $$\mathbf{d}_{	ext{in}}$$ represents an interpretable input feature (e.g., a dictionary feature from an autoencoder), one can compute $$\mathbf{d}_{	ext{out}} = (\mathbf{d}_{	ext{in}}\mathbf{W}_V)\mathbf{W}_O$$ and then analyze $$\mathbf{d}_{	ext{out}}$$ to see if it corresponds to a meaningful or predictable transformation of $$\mathbf{d}_{	ext{in}}$$. This helps to characterize the specific computation performed by the OV circuit.

## Attention Heads as Composable Meso-Circuits

Each attention head, with its distinct QK-defined attention pattern and its OV-defined information processing function, acts as a self-contained meso-circuit. As discussed in [Part 7]({% post_url 2025-05-25-mechanistic-interpretability-part-7 %}), these meso-circuits are the fundamental building blocks that, through mechanisms of composition (such as Q-composition, K-composition, and V-composition, detailed in [Part 3]({% post_url 2025-05-25-mechanistic-interpretability-part-3 %})), combine to form more complex, multi-layered macro-circuits. A prime example of such emergent complexity is the **induction head** circuit (which we will explore in detail in [Part 9]({% post_url 2025-05-25-mechanistic-interpretability-part-9 %})), typically formed by the synergistic composition of at least two simpler attention heads, such as a previous token head and a pattern-matching/copying head, to achieve basic in-context learning.

## Conclusion

Individual attention heads in Transformer models are not opaque, monolithic entities but rather decomposable computational circuits. Their Query-Key (QK) components are responsible for generating diverse attention patterns—ranging from fixed positional biases to dynamic content-based matching—while their Output-Value (OV) components determine how the thereby attended information is processed, transformed, or copied. By rigorously analyzing the mathematical properties of their constituent weight matrices ($$\mathbf{W}_Q, \mathbf{W}_K, \mathbf{W}_V, \mathbf{W}_O$$) and the effective matrices $$\mathbf{W}_{QK}$$ and $$\mathbf{W}_{OV}$$, we can identify and understand recurring functional motifs. These individual head circuits serve as the fundamental meso-circuits that, through composition, give rise to the sophisticated algorithmic capabilities observed in large Transformer models.

Next, we will examine one of the most celebrated and illustrative examples of such composed circuits: induction heads, and their role in the mechanics of in-context learning.

In [Part 9 - Induction Heads: The Mechanics of In-Context Learning]({% post_url 2025-05-25-mechanistic-interpretability-part-9 %})

---

## References and Further Reading

-   **Olah, C., Cammarata, N., Schubert, L., Goh, G., Petrov, M., & Carter, S.** (2020). [Zoom In: An Introduction to Circuits](https://distill.pub/2020/circuits/zoom-in/). *Distill*.
-   **Elhage, N., Nanda, N., Olsson, C., Henighan, T., Joseph, N., Mann, B., ... & Olah, C.** (2021). [A Mathematical Framework for Transformer Circuits](https://transformer-circuits.pub/2021/framework/). *Transformer Circuits Thread*.
-   **Olsson, C., Elhage, N., Nanda, N., Joseph, N., DasSarma, N., Henighan, T., ... & Olah, C.** (2022). [In-context Learning and Induction Heads](https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/). *Transformer Circuits Thread*. (Provides many examples of different head types).
-   **Voita, E., Talbot, D., Moiseev, F., Sennrich, R., & Titov, I.** (2019). [Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, Others Can Be Pruned](https://arxiv.org/abs/1905.09418). *ACL*. (Early work on identifying specialized head roles). 